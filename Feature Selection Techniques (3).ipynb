{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34468869",
   "metadata": {},
   "source": [
    "# Feature Selection Techniques in Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f599f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66e2b4",
   "metadata": {},
   "source": [
    "## Removing features with low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83527fbb",
   "metadata": {},
   "source": [
    "The VarianceThreshold method from scikit-learn is used for feature selection based on variance. It removes features whose variance does not meet a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12cc97c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Input features\n",
    "X = [[0, 0, 1], \n",
    "     [0, 1, 0], \n",
    "     [1, 0, 0], \n",
    "     [0, 1, 1], \n",
    "     [0, 1, 0], \n",
    "     [0, 1, 1]]\n",
    "\n",
    "# Initialize VarianceThreshold with threshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "\n",
    "# Fit and transform the input features\n",
    "selected_features = sel.fit_transform(X)\n",
    "\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8e284",
   "metadata": {},
   "source": [
    "## Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17689a8",
   "metadata": {},
   "source": [
    "These methods select features from the dataset irrespective of the use of any machine learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447e670",
   "metadata": {},
   "source": [
    "### Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6023fdf7",
   "metadata": {},
   "source": [
    "Information gain calculates the reduction in entropy from the transformation of a dataset. It can be used for feature selection by evaluating the Information gain of each variable in the context of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63eed20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: [3 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Calculate information gain for feature selection\n",
    "information_gain = mutual_info_classif(X, y)\n",
    "\n",
    "# selected features\n",
    "selected_features = np.argsort(information_gain)[::-1][:2]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c1df4",
   "metadata": {},
   "source": [
    "### Univariate feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f894cbf9",
   "metadata": {},
   "source": [
    "#### SelectKBest\n",
    "\n",
    "Selects the K highest scoring features based on univariate statistical tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4d338d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: [2 3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Feature selection using ANOVA F-value\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# selected features\n",
    "selected_features = np.where(selector.get_support())[0]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee3044",
   "metadata": {},
   "source": [
    "## SelectPercentile\n",
    "\n",
    "Selects a user-specified percentage of the highest scoring features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfeb026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "# Select top 10% features using chi-square test\n",
    "selector = SelectPercentile(score_func=chi2, percentile=10)\n",
    "X_new = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7fa6f",
   "metadata": {},
   "source": [
    "## SelectFpr\n",
    "\n",
    "Selects features based on a false positive rate criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c039d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFpr, chi2\n",
    "\n",
    "# Select features with false positive rate < 0.01 using chi-square test\n",
    "selector = SelectFpr(score_func=chi2, alpha=0.01)\n",
    "X_new = selector.fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b140234",
   "metadata": {},
   "source": [
    "##  SelectFdr\n",
    "\n",
    "Selects features based on a false discovery rate criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ac88055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFdr, chi2\n",
    "\n",
    "# Select features with false discovery rate < 0.01 using chi-square test\n",
    "selector = SelectFdr(score_func=chi2, alpha=0.01)\n",
    "X_new = selector.fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4016591",
   "metadata": {},
   "source": [
    "##  SelectFwe\n",
    "\n",
    "Selects features based on family wise error criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae50bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFwe, chi2\n",
    "\n",
    "# Select features with family wise error < 0.01 using chi-square test\n",
    "selector = SelectFwe(score_func=chi2, alpha=0.01)\n",
    "X_new = selector.fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf42d1d",
   "metadata": {},
   "source": [
    "## Generic Univariate Select \n",
    "\n",
    "Allows performing univariate feature selection with a configurable strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b71a2b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
    "\n",
    "# Select top 5 features using chi-square test\n",
    "selector = GenericUnivariateSelect(score_func=chi2, mode='k_best', param=5)\n",
    "X_new = selector.fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f80408b",
   "metadata": {},
   "source": [
    "## Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c955b20",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23bfd70",
   "metadata": {},
   "source": [
    "The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. \n",
    "\n",
    "First, The estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd0d2f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: [ True  True  True  True  True False False False False False]\n",
      "Feature ranking: [1 1 1 1 1 2 3 6 5 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate example data\n",
    "X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "\n",
    "# Create a linear regression estimator\n",
    "estimator = LinearRegression()\n",
    "\n",
    "# Create RFE selector\n",
    "selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    "\n",
    "# Fit RFE selector\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "# Selected features and ranks of features\n",
    "print(\"Selected features:\", selector.support_)\n",
    "print(\"Feature ranking:\", selector.ranking_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b9d7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: [2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Feature selection using RFE with Logistic Regression\n",
    "estimator = LogisticRegression()\n",
    "selector = RFE(estimator, n_features_to_select=2, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "# selected features\n",
    "selected_features = np.where(selector.support_)[0]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b75505",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination with Cross-Validation (RFECV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "182f0df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: [ True  True  True  True  True  True False False False False]\n",
      "Feature ranking: [1 1 1 1 1 1 2 5 4 3]\n",
      "Optimal number of features: 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate example data\n",
    "X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "\n",
    "# Create a linear regression estimator\n",
    "estimator = LinearRegression()\n",
    "\n",
    "# Create RFECV selector\n",
    "selector = RFECV(estimator, step=1, cv=5)\n",
    "\n",
    "# Fit RFECV selector\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "# Selected features\n",
    "print(\"Selected features:\", selector.support_)\n",
    "\n",
    "# Rank of features\n",
    "print(\"Feature ranking:\", selector.ranking_)\n",
    "print(\"Optimal number of features:\", selector.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1c078",
   "metadata": {},
   "source": [
    "## Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0208a3",
   "metadata": {},
   "source": [
    "### Feature selection using SelectFromModel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20869917",
   "metadata": {},
   "source": [
    "### L1-based feature selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c35caa",
   "metadata": {},
   "source": [
    "Linear models penalized with the L1 norm is  used to reduce the dimensionality of the data alongwith SelectFromModel to select the non-zero coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a963cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features shape: (442, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create LassoCV model\n",
    "lasso = LassoCV(cv=5, random_state=0)\n",
    "\n",
    "# Fit the LassoCV model\n",
    "lasso.fit(X_scaled, y)\n",
    "\n",
    "# Create SelectFromModel instance with the trained LassoCV model\n",
    "model = SelectFromModel(lasso, prefit=True)\n",
    "\n",
    "# Transform the original feature matrix to select features based on importance\n",
    "X_selected = model.transform(X_scaled)\n",
    "\n",
    "# Print selected features\n",
    "print(\"Selected features shape:\", X_selected.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24b51b",
   "metadata": {},
   "source": [
    "### Tree-based Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412368e7",
   "metadata": {},
   "source": [
    "Random forest regressors provide built-in feature importance through the feature_importances_ attribute. This attribute captures the average decrease in impurity across all trees in the forest indicating how much a feature contributes to improving the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5900e8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: [2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Feature selection using Random Forest feature importance\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y)\n",
    "\n",
    "# feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "selected_features = np.argsort(feature_importances)[::-1][:2]\n",
    "\n",
    "# selected features\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0e021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
